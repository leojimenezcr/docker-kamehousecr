services:

  # ollama and API
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    pull_policy: missing
    tty: true
    restart: unless-stopped
    networks:
      - ollama-net
    # Expose Ollama API outside the container stack
    #ports:
    #  - 8081:11434
    volumes:
      - type: bind
        source: /home/leojimenezcr/ollama/ollama
        target: /root/.ollama
    # GPU support (turn off by commenting with # if you don't have an nvidia gpu)
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: 1
    #          capabilities:
    #            - gpu

  # webui, nagivate to http://host:8080/ to use
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    pull_policy: missing
    volumes:
      - type: bind
        source: /home/leojimenezcr/ollama/ollama-webui
        target: /app/backend/data
    depends_on:
      - ollama
    networks:
      - ollama-net
    ports:
      - 8080:8080
    environment:
      - "OLLAMA_API_BASE_URL=http://ollama:11434/api"
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

networks:
  ollama-net:
